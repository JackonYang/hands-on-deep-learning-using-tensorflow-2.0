{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "实验结果：\n",
    "\n",
    "在 CIFAR10 数据集上的实验结果，使用 GPU GTX 1080.\n",
    "\n",
    "| 模型 | 正确率 | 运行时间 |\n",
    "| -- | -- | -- |\n",
    "| AlexNet | 70% | 45s |\n",
    "| VggNet | 78% | 500s |\n",
    "\n",
    "观点：\n",
    "\n",
    "1. VggNet 成功的把网络做的更深了，模型的算法性能也有了不错的提升。\n",
    "2. CIFAR10 数据集可能不足以体现 VggNet 的优势，需要换 ImageNet 试一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10 = tf.keras.datasets.cifar10\n",
    "(x_train, y_train),(x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_net_block(input_data, filters, conv_size):\n",
    "    x = layers.Conv2D(filters, conv_size, activation='relu', padding='same')(input_data)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(filters, conv_size, activation=None, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Add()([x, input_data])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, W, H, C = x_train.shape\n",
    "N_LABELS = 10\n",
    "\n",
    "inputs = tf.keras.Input(shape=(W, H, C))\n",
    "x = layers.Conv2D(32, 3, activation='relu')(inputs)\n",
    "x = layers.Conv2D(64, 3, activation='relu')(x)\n",
    "x = layers.MaxPooling2D(3)(x)\n",
    "\n",
    "num_res_net_blocks = 10\n",
    "for i in range(num_res_net_blocks):\n",
    "  x = res_net_block(x, 64, 3)\n",
    "\n",
    "x = layers.Conv2D(64, 3, activation='relu')(x)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!! start time counting\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 20s 400us/sample - loss: 1.5689 - accuracy: 0.4235 - val_loss: 1.3029 - val_accuracy: 0.5315\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 17s 333us/sample - loss: 1.1415 - accuracy: 0.5947 - val_loss: 1.4754 - val_accuracy: 0.4920\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 17s 331us/sample - loss: 0.9368 - accuracy: 0.6773 - val_loss: 1.3205 - val_accuracy: 0.5733\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 17s 337us/sample - loss: 0.7941 - accuracy: 0.7310 - val_loss: 0.9169 - val_accuracy: 0.6843\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 17s 334us/sample - loss: 0.7058 - accuracy: 0.7615 - val_loss: 0.8991 - val_accuracy: 0.7011\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 17s 335us/sample - loss: 0.6323 - accuracy: 0.7879 - val_loss: 0.8129 - val_accuracy: 0.7359\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 17s 342us/sample - loss: 0.5741 - accuracy: 0.8058 - val_loss: 0.7193 - val_accuracy: 0.7642\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 17s 341us/sample - loss: 0.5216 - accuracy: 0.8243 - val_loss: 0.6717 - val_accuracy: 0.7774\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 17s 336us/sample - loss: 0.4749 - accuracy: 0.8397 - val_loss: 0.7826 - val_accuracy: 0.7494\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 17s 339us/sample - loss: 0.4320 - accuracy: 0.8530 - val_loss: 0.6614 - val_accuracy: 0.7845\n",
      "10000/10000 - 1s - loss: 0.6614 - accuracy: 0.7845\n",
      "!! Total timecost: 173.83s\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "print('!! start time counting')\n",
    "tic = time.time()\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=10, \n",
    "                    validation_data=(x_test, y_test))\n",
    "model.evaluate(x_test, y_test, verbose=2)\n",
    "toc = time.time()\n",
    "print('!! Total timecost: %.2fs' % (toc - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_res_block(input_data, filters, conv_size):\n",
    "  x = layers.Conv2D(filters, conv_size, activation='relu', padding='same')(input_data)\n",
    "  x = layers.BatchNormalization()(x)\n",
    "  x = layers.Conv2D(filters, conv_size, activation='relu', padding='same')(x)\n",
    "  x = layers.BatchNormalization()(x)\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3x3 convolution\n",
    "def conv3x3(channels, stride=1, kernel=(3, 3)):\n",
    "    return layers.Conv2D(channels, kernel, strides=stride, padding='same',\n",
    "                               use_bias=False,\n",
    "                            kernel_initializer=tf.random_normal_initializer())\n",
    "\n",
    "class ResnetBlock(Model):\n",
    "\n",
    "    def __init__(self, channels, strides=1, residual_path=False):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "\n",
    "        self.channels = channels\n",
    "        self.strides = strides\n",
    "        self.residual_path = residual_path\n",
    "\n",
    "        self.conv1 = conv3x3(channels, strides)\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.conv2 = conv3x3(channels)\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "\n",
    "        if residual_path:\n",
    "            self.down_conv = conv3x3(channels, strides, kernel=(1, 1))\n",
    "            self.down_bn = layers.BatchNormalization()\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        residual = inputs\n",
    "\n",
    "        x = self.bn1(inputs, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        # this module can be added into self.\n",
    "        # however, module in for can not be added.\n",
    "        if self.residual_path:\n",
    "            residual = self.down_bn(inputs, training=training)\n",
    "            residual = tf.nn.relu(residual)\n",
    "            residual = self.down_conv(residual)\n",
    "\n",
    "        x = x + residual\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(Model):\n",
    "\n",
    "    def __init__(self, block_list, initial_filters=16, **kwargs):\n",
    "        super(ResNet, self).__init__(**kwargs)\n",
    "\n",
    "        self.num_blocks = len(block_list)\n",
    "        self.block_list = block_list\n",
    "\n",
    "        self.in_channels = initial_filters\n",
    "        self.out_channels = initial_filters\n",
    "        self.conv_initial = conv3x3(self.out_channels)\n",
    "\n",
    "        self.blocks = models.Sequential(name='dynamic-blocks')\n",
    "\n",
    "        # build all the blocks\n",
    "        for block_id in range(len(block_list)):\n",
    "            for layer_id in range(block_list[block_id]):\n",
    "\n",
    "                if block_id != 0 and layer_id == 0:\n",
    "                    block = ResnetBlock(self.out_channels, strides=2, residual_path=True)\n",
    "                else:\n",
    "                    if self.in_channels != self.out_channels:\n",
    "                        residual_path = True\n",
    "                    else:\n",
    "                        residual_path = False\n",
    "                    block = ResnetBlock(self.out_channels, residual_path=residual_path)\n",
    "\n",
    "                self.in_channels = self.out_channels\n",
    "\n",
    "                self.blocks.add(block)\n",
    "\n",
    "            self.out_channels *= 2\n",
    "\n",
    "        self.final_bn = keras.layers.BatchNormalization()\n",
    "        self.avg_pool = keras.layers.GlobalAveragePooling2D()\n",
    "        self.fc = keras.layers.Dense(num_classes)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "\n",
    "        out = self.conv_initial(inputs)\n",
    "\n",
    "        out = self.blocks(out, training=training)\n",
    "\n",
    "        out = self.final_bn(out, training=training)\n",
    "        out = tf.nn.relu(out)\n",
    "\n",
    "        out = self.avg_pool(out)\n",
    "        out = self.fc(out)\n",
    "\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!! start time counting\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 52s 1ms/sample - loss: 2.0852 - accuracy: 0.2730 - val_loss: 1.5258 - val_accuracy: 0.4365\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 49s 985us/sample - loss: 1.3984 - accuracy: 0.4961 - val_loss: 1.3332 - val_accuracy: 0.5622\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 49s 985us/sample - loss: 1.1125 - accuracy: 0.6151 - val_loss: 1.2190 - val_accuracy: 0.6067\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 49s 986us/sample - loss: 0.9724 - accuracy: 0.6727 - val_loss: 0.8789 - val_accuracy: 0.7029\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 49s 988us/sample - loss: 0.8501 - accuracy: 0.7203 - val_loss: 0.8442 - val_accuracy: 0.7317\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 49s 988us/sample - loss: 0.7547 - accuracy: 0.7558 - val_loss: 0.6965 - val_accuracy: 0.7688\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 49s 989us/sample - loss: 0.6710 - accuracy: 0.7815 - val_loss: 0.5969 - val_accuracy: 0.8003\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 49s 988us/sample - loss: 0.6104 - accuracy: 0.8026 - val_loss: 0.9500 - val_accuracy: 0.6948\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 49s 989us/sample - loss: 0.5583 - accuracy: 0.8185 - val_loss: 0.5496 - val_accuracy: 0.8187\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 49s 989us/sample - loss: 0.5114 - accuracy: 0.8349 - val_loss: 0.6729 - val_accuracy: 0.7891\n",
      "10000/10000 - 3s - loss: 0.6729 - accuracy: 0.7891\n",
      "!! Total timecost: 500.56s\n"
     ]
    }
   ],
   "source": [
    "# using Batch Norm\n",
    "\n",
    "cifar10 = tf.keras.datasets.cifar10\n",
    "(x_train, y_train),(x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "N, W, H, C = x_train.shape\n",
    "N_LABELS = 10\n",
    "initial_filters=16\n",
    "\n",
    "weight_decay = 0.000\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(16, (3, 3), strides=1, padding='same',\n",
    "                 input_shape=(W, H, C), kernel_initializer=tf.random_normal_initializer()))\n",
    "\n",
    "\n",
    "# //////// starting\n",
    "# 1st max pooling. 2 conv layer\n",
    "model.add(layers.Conv2D(64, (3, 3), padding='same',\n",
    "                 input_shape=(W, H, C), kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.3))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 2nd max pooling. 2 conv layer\n",
    "model.add(layers.Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.4))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 3rd max pooling. 3 conv layer\n",
    "model.add(layers.Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.4))\n",
    "\n",
    "model.add(layers.Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.4))\n",
    "\n",
    "model.add(layers.Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 4th max pooling. 3 conv layer\n",
    "model.add(layers.Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.4))\n",
    "\n",
    "model.add(layers.Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.4))\n",
    "\n",
    "model.add(layers.Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 5th max pooling. 3 conv layer\n",
    "model.add(layers.Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.4))\n",
    "\n",
    "model.add(layers.Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.4))\n",
    "\n",
    "model.add(layers.Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "# 2 Fully Connected Layers\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(N_LABELS, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "print('!! start time counting')\n",
    "tic = time.time()\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=10, \n",
    "                    validation_data=(x_test, y_test))\n",
    "model.evaluate(x_test, y_test, verbose=2)\n",
    "toc = time.time()\n",
    "print('!! Total timecost: %.2fs' % (toc - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train vs evaluate 的 accuracy 曲线\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
